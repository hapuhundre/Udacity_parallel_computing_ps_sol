# Udacity Intro to Parallel Computing

course page: https://classroom.udacity.com/courses/cs344

course wiki: 

my problem set solution: 



## Lecture 2 GPU Hardware & Parallel Communication Patterns

### åŸºæœ¬é€šä¿¡æ–¹å¼

**map**: one to one ä»task åˆ°å†…å­˜

**gather**: èšåˆï¼Œä¾‹å¦‚å·ç§¯ã€blur

**scatter**: å¾—åˆ°ç»“æœï¼Œå‘æ•£ç»™å…¶ä»–å†…å­˜å•å…ƒï¼Œä¾‹å¦‚sort

**stencil**: ä»»åŠ¡ç‹¬ç«‹ï¼Œä½†å†…å­˜æœ‰é‡å ï¼ˆonly read)ã€‚æ³¨æ„ï¼Œè¿™ç§å’ŒgatheråŒºåˆ«åœ¨äºstencilæœ‰æ¨¡æ¿å®šä¹‰ï¼Œæ²¡æœ‰å‰è€…çµæ´»ã€‚

**transpose**: å†…å­˜çŸ©é˜µçš„è½¬ç½®,ä¾‹å¦‚å°†è®¸å¤šstructä¸­çš„floatæˆå‘˜å˜é‡é›†ä¸­åœ¨ä¸€èµ·ã€‚

åé¢è¿˜ä¼šæ¥è§¦åˆ°reduceï¼Œscan/sortç­‰èŒƒå¼ã€‚



### GPUç¡¬ä»¶

GPUè´Ÿè´£æŠŠBlocksåˆ†é…ç»™SM,åŒæ—¶GPUæ— æ³•ä¿è¯blocksä¹‹é—´åŒæ—¶è¿è¡Œæˆ–ä¾æ¬¡è¿è¡Œã€‚

CUDAæä¾›äº†ä¸€ç§ä¸ä¾èµ–äºSMæ•°é‡çš„å®ç°ã€‚ä½†blockä¹‹é—´çš„é€šä¿¡æ˜¯ç¼ºå¤±çš„ã€‚CUDAæä¾›ä»¥ä¸‹ä¿è¯ï¼š

- a blockåœ¨ä¸€ä¸ªSMä¸Šè¿è¡Œï¼›

- ä¸€ä¸ªkernelåˆ°ä¸‹ä¸€ä¸ªkernelæ—¶ï¼Œè¯¥kernelçš„æ‰€æœ‰blockå¿…é¡»å®Œæˆè®¡ç®—ã€‚

  

memory model:

![](memory_model.png)



CUDAä¸­çš„åŒæ­¥æœºåˆ¶ï¼š

- Barrier: `_syncthreads();`

ä¾‹å¦‚ä¸‹è¿°ä»£ç ï¼Œarrayåˆå§‹åŒ–å¹¶å·¦ç§»çš„æ“ä½œ

```c++
int idx = threadIdx.x;
__shared__ int array[128];
array[idx] = threadIdx.x;
if(idx < 127) {
    array[idx] = array[idx+1];
}
```

éœ€è¦ä¸‰ä¸ªåŒæ­¥æ“ä½œï¼ˆæ¯ä¸€æ¬¡read/writeæ“ä½œéƒ½éœ€è¦sync)ï¼Œä¿®æ”¹åçš„ä»£ç å¦‚ä¸‹ï¼š

```c++
int idx = threadIdx.x;
__shared__ int array[128];
array[idx] = threadIdx.x;
__syncthreads();
if(idx < 127) {
    int temp = array[idx+1]; // ä»»ä½•è¯»å†™éƒ½åº”è¯¥æ‹†å¼€ã€‚
    __syncthreads();
    array[idx] = temp;
    __syncthreads();
}
```



GPUæ€§èƒ½ä¼˜åŒ–å¤§çš„æ–¹å‘æ˜¯ï¼š

- Math / Memory, æ¯ä¸ªçº¿ç¨‹è®¡ç®—è¦å¤šï¼Œè®¿é—®å†…å­˜çš„æ—¶é—´è¦å°‘ã€‚

ç§»åŠ¨é¢‘ç¹è®¿é—®çš„å†…å­˜åˆ°æ›´å¿«çš„å±‚çº§ä¸Šï¼šlocal > shared >> global >> host(cpu)

- coalesce global memory accesses

  ç›¸é‚»threadè®¿é—®ç›¸é‚»çš„å†…å­˜åœ°å€

  ![](coalesce.png)



GPUåŸå­æ“ä½œï¼šä¾‹å¦‚1e6çº¿ç¨‹å¾€é•¿åº¦ä¸º10çš„arrayå†™å…¥æ•°æ®ã€‚

```c++
int i = blockIdx.x*blockDim.x + threadIdx.x;
i = i%10;
atomicAdd(&g[i], 1); // add one
```

é™åˆ¶ï¼šä»…æœ‰éƒ¨åˆ†è¿ç®—ç¬¦ï¼Œä¾‹å¦‚ä¸æ”¯æŒ!,powï¼Œ%dç­‰ï¼Œmore to see atomic CAS()ï¼› no ordering constraints; serializes access(SLOW!)



thread divergence:

ä¾‹å¦‚çº¿ç¨‹åœ¨if/elseä¸‹é¢è¢«åˆ†å‰²æˆä¸¤ä¸ªbatchï¼Œç„¶åæ‰§è¡Œé¡ºåºè¢«å‘æ•£ã€‚loopä¹Ÿä¼šå¯¼è‡´çº¿ç¨‹å‘æ•£ï¼Œä¾‹å¦‚`for(int i=0; i<=threadIdx.x;++i)`ã€‚



## Lecture 3: GPU Algo(Reduce, Scan, Histogram)

### Reduce

eg: 1+2+3+4

reduction operator: å¿…é¡»æ˜¯äºŒå…ƒè¿ç®—ç¬¦ï¼Œè€Œä¸”éœ€è¦æ»¡è¶³äº¤æ¢å¾‹

ä¸²è¡Œå®ç°reduceæ“ä½œä¸€èˆ¬forå¾ªç¯å®Œäº‹ï¼Œå¤æ‚åº¦$O(n)$.æ‰€è°“çš„parallel reduceå°±æ˜¯ ((a+b) + c) +d è½¬å˜ä¸º (a+b) + (c+d)ã€‚

more to see: [Brent's Theorem](https://zhuanlan.zhihu.com/p/63351764)

reduce æ±‚å’Œï¼š

```c++
__global__ void shmem_reduce_kernel(float *d_out, const float *d_in)
{
    extern __shared__ float sdata[]; // å¤§å°åœ¨kernelè°ƒç”¨çš„æ—¶å€™ç¡®å®šã€‚
    int threadId = threadIdx.x + blockDim.x * blockIdx.x;
    int tid = threadIdx.x;
    
    sdata[tid]  = d_in[threadId];
    __syncthreads();
    
    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1)
    {
        if(tid < s)
        {
            sdata[tid] += sdata[tid + s]; // ç­‰ä»·äºarray[i] += array[i+ARRAY_SIZE/2];
        }
        __syncthreads();
    }
    // only thread 0 writes result for this block back to global mem
    if (tid == 0)
    {
        d_out[blockIdx.x] = sdata[0];
    }
}
```

å½“ç„¶ï¼Œsharedç‰ˆæœ¬çš„kernelå¹¶æœªå……åˆ†åˆ©ç”¨å†…å­˜ï¼Œè¿˜å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚



### Scan

PS: è¯¥èŠ‚å‡ºç°äº†step / workçš„æ¦‚å¿µï¼Œå…¶å®å°±æ˜¯éœ€è¦å¤šå°‘ä¸ªå•å…ƒè¿›è¡Œå¤„ç†ï¼ˆstepï¼‰ä»¥åŠæ€»çš„ç®—æ³•å¤æ‚åº¦ã€‚

ä¾‹å¦‚1,2,3,4,5,6ï¼Œç±»ä¼¼äº`std::partial_sum`,ç»“æœä¸º`1,3,6,10,15,21`ã€‚åœ¨æ•°æ®å‹ç¼©ï¼Œç¨€ç–çŸ©é˜µè®¡ç®—ä¸­æœ‰æ‰€ä½¿ç”¨ã€‚

scanç±»é—®é¢˜å’Œreduceä¸€æ ·ï¼ŒèŒƒå¼å¦‚ä¸‹ï¼š

- input array
- binary associate operator
- identity element, å½¢å¼ä¸º[I op a = a]ï¼Œscané—®é¢˜è¿˜è¦æ±‚è¿ç®—ç¬¦æ”¯æŒå…·å¤‡å•å…ƒå…ƒç´ ï¼Œä¾‹å¦‚ä¹˜æ³•çš„Iä¸º1ï¼ŒåŠ æ³•ä¸º0, orä¸ºfalse, and ä¸ºtrue.

scané—®é¢˜å¯è¡¨è¾¾ä¸ºï¼š

[a0, a1,a2, .., an] -> [I, a0, a0 op a1, a0 op a1 op a2, ...  ]

ä¸Šè¿°é—®é¢˜å¯èƒ½å¯¼è‡´æœ€åä¸€ä¸ªæ•°æ²¡æœ‰è¢«æ“ä½œï¼Œä¾‹å¦‚[3,3,1,5,9]å–æœ€å¤§å€¼(æœ€å°å€¼IEä¸º0)ï¼Œoutputä¸º[0,3,3,3,5],ä¸åŒ…æ‹¬9ã€‚è¿™ç§scanæ–¹å¼å«exclusive scanï¼Œä¸åŒ…æ‹¬å½“å‰å€¼ã€‚è€Œinclusive scançš„ç»“æœä¸º[3,3,3,5,9]ï¼ŒåŒ…æ‹¬å½“å‰å€¼ã€‚

Hillis & Steels Scan: 

![](hillis_steele_scan.PNG)

å¯ä»¥çœ‹å‡ºï¼Œè¯¥scanç®—æ³•å¤æ‚åº¦ä¸º$nlog(n)$,å³ä¸Šå›¾ä¸­é•¿æ–¹å½¢çš„é¢ç§¯ã€‚è€Œæ€»å…±éœ€è¦$log(n)$æ¬¡è¿­ä»£ã€‚



Blelloch Scan

![](blelloch.png)

15-418 Lab2ä¸­çš„scanä½¿ç”¨çš„æ˜¯è¯¥æ–¹æ³•ï¼Œç”±up_sweepå’Œdown_sweepä¸¤éƒ¨åˆ†ç»„æˆã€‚è¯¥æ–¹æ³•ä½¿ç”¨çš„æ›´å¤šçš„steps,ä½†å…¶æ­¥éª¤åƒä¸€ä¸ªç›¸å¯¹çš„ä¸¤ä¸ªä¸‰è§’å½¢(â³)ï¼Œè€Œä¸æ˜¯é•¿æ–¹å½¢ã€‚å¯¹äºå¤§é‡é•¿åº¦çš„scané—®é¢˜ï¼Œblellochæ–¹æ³•æ€»æ—¶é—´æ›´å°ã€‚



### Histogram

ä¾‹å¦‚å¯¹æ—¶é—´åºåˆ—çš„æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œå½¢æˆç±»ä¼¼äºæ ‘çŠ¶å›¾çš„ç»“æœã€‚

ä¸²è¡Œçš„histogramä»£ç å¦‚ä¸‹ï¼š

```c++
for(const auto& d : data_set){
    result[classify(d)]++; // classifyä¸ºæ•°æ®åˆ†ç±»å‡½æ•°
}
```

æ˜¾ç„¶ï¼Œresultä½œä¸ºglobalæ•°æ®ï¼Œæ˜¯å¹¶è¡ŒåŒ–å¤„ç†çš„å…³é”®ã€‚PS: å°†blockå†…çš„æ•°æ®sharedï¼Œç„¶åå†æ±‚å’Œï¼Ÿè¯¾ç¨‹æå‡ºäº†ä¸‰ç§æ–¹æ³•ï¼š

1. åŸå­æ“ä½œ

   ```c++
   atomicAdd(&result[classified_res],1);
   ```

   è¯¥æ–¹æ³•çš„å¹¶è¡Œspeedupä¸ä¼šå¤ªå¥½ï¼Œå°¤å…¶æ˜¯resultçš„bucketè¾ƒå°æ—¶ã€‚

2. æ¯ä¸ªthreadæ‹¥æœ‰è‡ªå·±çš„result (local memory), ç„¶åå¯¹æ¯ä¸ªthreadçš„resultè¿›è¡Œreduceæ“ä½œã€‚

3. å°†dataå˜æˆhash(keyä¸ºdata, valueä¸º1ï¼Œreduce flag)ã€‚ç„¶åæ’åºï¼ˆåé¢ä¼šè®¨è®ºï¼‰ã€‚



### PS3 tone mapping

æ•´ä¸ªçš„flowæ˜¯reduce->histogram->prefix_sumã€‚æ³¨æ„ä»¥ä¸‹äº‹é¡¹ï¼š

- æœ¬åœ°åŸåˆ™ï¼Œä¸è¦åœ¨å½“å‰threadIdå†™è¿™ç±»è¯­å¥`sh_data[threadId+offset] += sh_data[threadId]`;

- èµ‹å€¼æ³¨æ„åŒæ­¥ï¼Œä¾‹å¦‚å°†ä¸Šè¿°è¯­å¥è¿˜æœ‰å¦ä¸€ä¸ªé—®é¢˜ï¼Œä¸èƒ½ä¸åŒæ­¥çš„æ—¶å€™å°±å–ç”¨ï¼Œåº”è¯¥æ”¹æˆ

```c++
   for (unsigned int offset = 1; offset <= numBins / 2; offset <<= 1)
   {
      int left = threadId - offset;
      int left_val = 0;
      if (left >= 0)
         left_val = s_cdf[left]; // è¯»lock
      __syncthreads();
      if(left >= 0)
         s_cdf[threadId] += left_val; // å†™lock
      __syncthreads();
   }
```



## Lecture 4: GPU Algo(Sort, Scan)



filterï¼š

predicate -> scan -> scatterï¼ˆèšåˆï¼‰ï¼Œç±»ä¼¼ä¸map-reduce,å°†æ•°æ®è¿‡æ»¤åå‹ç¼©æˆæ›´çŸ­çš„é›†åˆã€‚



åˆ†æ®µexclusive scan:

å°±æ˜¯å°†æ•°ç»„åˆ†æ®µï¼Œç„¶åå„è‡ªprefix sum.

### Spmv

SpMv: ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼Œä¾‹å¦‚PageRankï¼Œweb Rï¼ˆæ¨ªè½´ï¼‰ -> web Cï¼ˆçºµè½´ï¼‰ã€‚

Compressed Sparse Row: CSRè¡¨ç¤ºæ³•

ä¾‹å¦‚ï¼š

```
a 0 b    x
c d e  * y
0 0 f    z
```

éé›¶å€¼ value: [a,b,c,d,e,f]

è¿™äº›éé›¶å€¼æ¥è‡ªå“ªä¸ªcolumn: [0,2,0,1,2,1]

åŒæ ·çš„ROWPTR: [0,2,5],æ³¨æ„ï¼Œè¿™é‡Œçš„rowä¸ºå•¥3ä¸ªå°±å¤Ÿäº†ï¼Ÿæˆ‘ä»¬çŸ¥é“ï¼Œmatrixæ˜¯n*nçš„ï¼Œç„¶ååªéœ€è¦çŸ¥é“æ¯ä¸€è¡Œç¬¬ä¸€ä¸ªéé›¶å€¼åœ¨valueä¸­çš„indexå³å¯ã€‚

![](spmv.png)

å¾ˆæ¸…æ™°ï¼Œæ€ä¹ˆç”¨CUDAå†™å‘¢ï¼ŸğŸ˜„

ref: https://zhuanlan.zhihu.com/p/383115932



### Sort

brick sort: ä¸¤ä¸¤æˆå¯¹ï¼Œç„¶åæ’åº.ç®—æ³•å¤æ‚åº¦O(n^3)

eg: [5,1], [4, 2] 3  ->  [1, 5],[2,4],3 -> 1,[2,5],[3,4]->[1,2],[3,5],4->1,2,3,4,5



merge sort:

åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µï¼Œä¸€ä¸ªçº¿ç¨‹èƒ½holdä½çš„é˜¶æ®µï¼Œä¸€ä¸ªblockèƒ½holdä½çš„é˜¶æ®µï¼ˆtask per block), final merge

è¿™é‡Œæœ‰ä¸ªé—®é¢˜ï¼Œå¦‚ä½•parallel merge? ä¾‹å¦‚ä¸¤ä¸ªæœ‰åºarrayåˆå¹¶ã€‚

æ¯ä¸€ä¸ªarrayå·²ç»çŸ¥é“å®ƒè‡ªå·±çš„indexï¼Œç„¶åå»å…¶ä»–arrayäºŒåˆ†æŸ¥æ‰¾å…¶lower_boundå³å¯ã€‚

ç„¶è€Œï¼Œmerge sortæœ€éš¾çš„æ˜¯final mergeï¼Œå¤§é‡çš„æ•°æ®ï¼Œbatchæ•°é‡å¾ˆå°‘ã€‚å¦‚ä½•å°†å…¶åˆ†è§£ç»™å…¶ä»–SMå‘¢ï¼Ÿ

ä¾‹å¦‚[1~432432], [3~435436]æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ•°ç»„ç»§ç»­åˆ†è§£æˆå°çš„å—ï¼Œç„¶åè¿›è¡Œåˆå¹¶ã€‚ä¸¤ä¸ªåˆ—è¡¨çš„sub chunkçš„èµ·ç‚¹å’Œç»ˆç‚¹æ˜¯ä¸€è‡´çš„ã€‚



**sorting networks**

æ ¸å¿ƒæ€è·¯ï¼šä¿è¯å¹¶è¡Œswap

å†’æ³¡

ä½†éœ€è¦å¤šæ¬¡æ¯”è¾ƒï¼Œå› æ­¤æ— è®ºæ•°æ®æ’åºè´¨é‡å¦‚ä½•ï¼Œæ—¶é—´å¤æ‚åº¦ä¸€æ ·ã€‚not good



**radix sort**

åŸºæ•°æ’åºï¼ŒäºŒè¿›åˆ¶ç”±ä½ä½åˆ°é«˜ä½ä¾æ¬¡æ’åºã€‚0/1åˆ†ç±»å°±æ˜¯ä¸Šé¢æåˆ°çš„compactæ“ä½œã€‚

GPUä¸Šå¾ˆå¿«ã€‚



**quick sort**

å¤ä¹ ä¸‹ï¼Œå†™ä¸ªä¸²è¡Œçš„quick sort:

```c++
// todo
```

GPUä¸æ”¯æŒé€’å½’ï¼Œä½†æ”¯æŒkernel å»launchå…¶ä»–kernel



### PS4 radix sort of CUDA

å…ˆçœ‹ä¸‹CPUç‰ˆæœ¬çš„radix sortï¼ˆreference_calculation),å¾ˆå®¹æ˜“ç†è§£ã€‚

GPUç‰ˆæœ¬çš„æµç¨‹å¦‚ä¸‹ï¼š

![](https://developer.nvidia.com/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/39fig14.jpg)

more to see: https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda

å›¾ä¸­`totalFalses`å°±æ˜¯0/1çš„histogram[0]ã€‚

æŒ‰ç…§ä¸Šå›¾ä¸­çš„flowå†™ä¸‹æ¥ï¼ŒåŸºæœ¬æ²¡å•¥é—®é¢˜ï¼Œéš¾åº¦ä¸å¤§ã€‚ï¼ˆä½†å¦‚æœæ²¡æœ‰ä¸Šå›¾ä½œä¸ºå‚è€ƒï¼Œåœ°ç‹±éš¾åº¦ğŸ˜‚ï¼‰



## Lecture 5: Optimizing GPU Programs 

GPUæœ‰æ•ˆè®¡ç®—çš„å‡ ä¸ªåŸåˆ™ï¼š

- æœ€å¤§åŒ–ç®—æœ¯å¯†åº¦ï¼ˆarithmetic intensity)
- å‡å°‘å†…å­˜ç›¸å…³çš„æ“ä½œï¼ŒåŒ…æ‹¬å…¨å±€å†…å­˜çš„è¯»å–æ¬¡æ•°
- é¿å…thread divergence

### GPUä¼˜åŒ–

1. better algorithmsï¼›
2. æ»¡è¶³ä¸Šè¿°æœ‰æ•ˆè®¡ç®—çš„åŸåˆ™ï¼›
3. arch-specific detailed optimizations, ä¾‹å¦‚vector register-SSE, AVX, L1 cacheä¼˜åŒ–ç­‰ï¼›
4. æŒ‡ä»¤é›†ä¼˜åŒ–ã€‚

å¯¹äºGPUè€Œè¨€ï¼Œ1~2èƒ½è·å¾—3~10Xçš„åŠ é€Ÿæ¯”ï¼Œæ˜¯ä¼˜åŒ–çš„é‡ç‚¹ã€‚è€Œ3å¸¦æ¥çš„å¥½å¤„ä¸€èˆ¬åœ¨30%~80%ä¹‹é—´ã€‚CPUåˆ™ä¸åŒï¼Œç¬¬ä¸‰ç±»ä¼˜åŒ–å¯ä»¥å¸¦æ¥è¾ƒå¤§çš„æ€§èƒ½æå‡ã€‚



#### APOD:ä¼˜åŒ–æµç¨‹

analyze -> parallelize -> optimize -> deploy, å¾ªç¯ã€‚

parallelizeæ–¹å¼ä¸€èˆ¬æœ‰ï¼šç”¨librariesï¼Œdirectives(openMP, OpenACC), æ›´æ¢ç¼–ç¨‹è¯­è¨€ï¼ˆCUDA, ISPC)

Optimize: profile-driven optimization, å¤šæµ‹å¤šåˆ†æ



weak scaling / strong scaling: 
1.strong scaling: ä½¿é—®é¢˜è§„æ¨¡ä¿æŒä¸å˜ï¼Œå¢åŠ å¤„ç†å™¨æ•°é‡ï¼Œç”¨äºæ‰¾åˆ°è§£è¯¥é—®é¢˜æœ€åˆé€‚çš„å¤„ç†å™¨æ•°é‡ã€‚å³æ‰€ç”¨æ—¶é—´å°½å¯èƒ½çŸ­è€Œåˆä¸äº§ç”Ÿå¤ªå¤§çš„å¼€é”€ã€‚ç»˜åˆ¶å¦‚ä¸‹å›¾å½¢ï¼š

![](https://img-blog.csdnimg.cn/20200628105342319.png)

2.weak scaling: è®©é—®é¢˜è§„æ¨¡ï¼ˆè®¡ç®—é‡ï¼‰éšå¤„ç†å™¨æ•°é‡å¢åŠ è€Œå¢åŠ ã€‚ç†æƒ³æƒ…å†µï¼š



![](https://img-blog.csdnimg.cn/20200628105405879.png)

strong scalingçš„ç»“æœè¾ƒéš¾è¾¾åˆ°ï¼Œå› ä¸ºéšç€å¤„ç†å™¨æ•°é‡çš„å¢åŠ é€šä¿¡å¼€é”€æˆæ¯”ä¾‹ä¸Šå‡ï¼›è€Œweak scalingçš„ç»“æœè¾ƒå®¹æ˜“è¾¾åˆ°ã€‚

refï¼šhttps://blog.csdn.net/Cobb141/article/details/106994450

https://www.kth.se/blogs/pdc/2018/11/scalability-strong-and-weak-scaling/



#### Analyze: profiling

åˆ†æåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šæ‰¾hotspots,å¸¸ç”¨çš„profileræœ‰ï¼šgprop, vTune, VerySleepy.  

"Amdahl's Law" åŠ é€Ÿæ¯”å’Œå¯å¹¶è¡ŒåŒ–çš„æ¯”ä¾‹æœ‰å…³ã€‚



### Parallelize: ä¸€ä¸ªçŸ©é˜µè½¬ç½®çš„ä¾‹å­

æŸ¥è¯¢è®¾å¤‡ä¿¡æ¯ï¼š

`/usr/local/cuda-10.2/samples/1_Utilities/deviceQuery`

ç¼–è¯‘åè¿è¡Œ



ä¾‹å¦‚æˆ‘çš„MX150,

Memory clock: 3004 Mhz = 3004*10^6 clocks/sec

Memory Bus Width: 64-bit = 8bytes

å…¶ç†è®ºçš„å¸¦å®½å³°å€¼ä¸ºï¼š37.55 GB/s, 

å¸¦å®½å³°å€¼ï¼š75% excellent 60~75% good 40~60% okay, 



ä¾‹å¦‚é•¿åº¦ä¸º1024çš„çŸ©é˜µè½¬ç½®ï¼Œå…¶å¸¦å®½å³°å€¼ä¸ºï¼š

1024*1024 * 4(å››ä¸ªå­—èŠ‚) * 2(è¯»å†™ä¸¤æ¬¡) / 0.67ms = 1.25*10^10, 12.5GB/sã€‚



cudaçš„å†…å­˜å¯¹é½ï¼šcoalescing(418æåˆ°è¿‡è¿™ä¸ªæ¦‚å¿µï¼Œç±»ä¼¼äºå†…å­˜å¯¹é½)



å¯ä»¥ä½¿ç”¨nsignt Eclipseï¼ˆLinux, Mac)æ£€æµ‹å³°å€¼å¸¦å®½ã€‚è‡³äºè¯¾ä¸­æåˆ°çš„NVVPï¼Œå’ŒVisual Profiler åé¢å°†ä¼šå¼ƒç”¨ï¼Œå…¶åŠŸèƒ½è¢«é›†æˆåˆ°Nsightä¸­ã€‚





**é—®é¢˜**ï¼šCUDAä¸­è¿›è¡ŒçŸ©é˜µè½¬ç½®çš„æœºåˆ¶å¯ä»¥æè¿°ä¸‹å—ï¼Ÿ

ref:

https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/

- Shared Memory Bank Conflicts
- padded shared memory(google it~)





#### Shared Memory

~~ä½œè€…ç»™äº†ä¸€ä¸ªä½¿ç”¨shared memoryå±€éƒ¨è½¬ç½®çš„ä¾‹å­ï¼Œæ¯”è¾ƒç®€å•ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸ªcaseä¹Ÿæ˜¯é”™è¯¯çš„ã€‚å› ä¸ºcudaæ— æ³•ä¿è¯blocksä¹‹é—´çš„æ‰§è¡Œé¡ºåºã€‚~~



å¯¹äºä½¿ç”¨çº¿ç¨‹åŒæ­¥çš„ä»£ç è€Œè¨€ï¼Œå‡å°‘threads per blocksçš„æ•°é‡èƒ½å‡å°‘æ¯ä¸ªçº¿ç¨‹çš„å¹³å‡ç­‰å¾…æ—¶é—´ã€‚åŒæ—¶å¢åŠ æ¯ä¸ªSMä¸­çš„blocksæ•°é‡ä¹Ÿå¯ä»¥è¾¾åˆ°è¯¥æ•ˆæœï¼ˆwhy?)



æ¯ä¸ªSMä¸Šçš„blocksæ•°é‡å’Œå…±äº«å†…å­˜å¤§å°éƒ½æ˜¯æœ‰é™çš„ã€‚

ä¾‹å¦‚æˆ‘æ‰‹å¤´ä¸Šè¿™ä¸ªåƒåœ¾MT150:

- Maximum number of threads per multiprocessorï¼š2048
- threads per block: 1024
- registers for all threads: 65536
- shared memory per block: 49152 bytes

æ˜¾ç„¶ï¼Œæ¯ä¸ªSMä¸Šçš„çº¿ç¨‹æ•°é‡æ˜¯æœ€ä¸ºç´§ä¿çš„è®¡ç®—èµ„æº



**è®¡ç®—èµ„æºçš„æœ€å¤§åŒ–åˆ©ç”¨æœ‰ä¸¤ä¸ªå…³é”®æ–¹å‘ï¼š**

- **å‡å°‘åŒæ­¥é€ æˆçš„ç­‰å¾…æ—¶é—´ minimize time waiting at barriers**
- **é¿å…çº¿ç¨‹åˆ†åŒ– thread divergence**



#### Thread Divergence

warp: set of threads that execute the same instruction at a time;

SIMD: CPU(SSE/AVX vector registers)

SIMT: GPU



thread divergenceå¸¦æ¥çš„åæœï¼š

- æ¯ä¸ªwarpæœ‰32ä¸ªçº¿ç¨‹ï¼Œæœ€å¤šå¸¦æ¥32å€çš„å‡é€Ÿæ¯”

- thread divergenceé—®é¢˜å¾€å¾€éœ€è¦ä¸€ä¸ªæ–°çš„ç®—æ³•æ¥è§£å†³ã€‚

- ä»£ç ä¸­å°½é‡é¿å…boundary check



tips:

- å¦‚æœéå¿…é¡»ï¼Œé¿å…ç”¨double

```c++
float a= b + 2.5;  // slower
float a = b + 2.5f // quicker
```

- å°½é‡ä½¿ç”¨cudaå†…ç½®çš„å‡½æ•°

```
__sin()
__cos()
__exp()
```

- å¦‚æœä¸æ€¥ç€ç”¨ï¼Œå¯ä»¥ä½¿ç”¨`cudaMemcpyAsync()`ï¼Œå‡å°‘é˜»å¡



#### Steams

ä½¿ç”¨cudaMemcpyAsync()å¯ä»¥å®ç°ç±»ä¼¼å¹¶å‘çš„æ•ˆæœï¼Œè¾¹è®¡ç®—è¾¹æ‹·è´æ•°æ®ã€‚



### PS5 histogramä¼˜åŒ–

ref: 

https://github.com/NVIDIA/thrust/blob/main/examples/histogram.cu

Programming  Massively Parallel  Processors (book PMPP) Chapter 9



æ¯”è¾ƒä¸‹PMPP Chapter9ä¸­çš„å®ç°ï¼Œå¯¹æ¯”å…¶è®¡ç®—æ—¶é—´ã€‚

| ä»£ç                     | è®¡ç®—æ—¶é•¿ |
| ----------------------- | -------- |
| CPU                     | 25.22ms  |
| atomic add              | 3.536ms  |
| book PMPP: strategy I   | 3.38ms   |
| book PMPP: strategy II  | 3.521ms  |
| book PMPP: strategy III | 2.313ms  |
| book PMPP: strategy IV  | 2.45ms   |

åŸºäºæ’åºçš„æ–¹æ³•å¯ä»¥å‘ç°ï¼Œæ’åºæ¶ˆè€—çš„æ—¶é—´å¤§æ¦‚ä¸º26mså·¦å³ï¼Œæ˜¾ç„¶åœ¨1024*10000è¿™ä¸ªæ•°ç»„é‡çº§ä¸‹ï¼ŒåŸºäºæ’åºçš„æ–¹æ³•æ— æ³•æ»¡è¶³è¦æ±‚ã€‚

- book PMPP: strategy I & II, æœ¬è´¨ä¸Šå’Œatomic add ç­‰ä»·çš„ï¼Œåªæ˜¯ç»™å‡ºäº†çº¿ç¨‹æ•°é‡æœ‰é™ï¼ˆä¾‹å¦‚è¿œå°äºarrayé•¿åº¦ï¼‰çš„è§£æ³•ã€‚
- book PMPP: strategy III:  shared memoryã€‚è¦ä»”ç»†è€ƒè™‘ä¸‹TPBè¿œå°äºnumBinsçš„æƒ…å†µã€‚å› æ­¤è§£å†³æ–¹æ¡ˆæ˜¯æ¯ä¸ªthreadéƒ½è¦å¤„ç†

æ³¨æ„ä¸‹ï¼š

```c++
for (unsigned int i = tid; i < num_elements; i += blockDim.x*gridDim.x)
```

ç­‰ä»·äºifè¯­å¥æº¢å‡ºçš„threadæå‰é€€å‡ºï¼Œä½†å¯ä»¥ç¼“è§£threads divergenceã€‚

è€Œä¸‹è¿°çš„ï¼š

```c++
  for(unsigned int binIdx = threadIdx.x; binIdx < numBins; binIdx += blockDim.x){
    s_histo[binIdx] = 0u;
  }
```

ç­‰ä»·äºäºå¤šä¸ªçº¿ç¨‹å¹¶è¡Œåœ°å»åˆå§‹åŒ–å…±äº«å†…å­˜ï¼Œå—¯ï¼Œæ¯”è¾ƒå·§å¦™~ã€‚

- book PMPP: strategy IV: ä¹¦ä¸­çš„ä»£ç æœ‰äº›é”™è¯¯ï¼Œæ”¹è¿‡æ¥äº†ã€‚æ²¡ä»€ä¹ˆç‰¹åˆ«çš„ï¼Œå°±æ˜¯é’ˆå¯¹å¦‚æœæ•°ç»„æŸä¸ªåŒºé—´æœ‰å¤§é‡ç›¸åŒçš„å€¼ï¼ˆä¾‹å¦‚å›¾ç‰‡çš„ç™½äº‘ï¼‰ï¼Œé€šè¿‡cacheæ“ä½œå¯ä»¥æé«˜ååé‡ï¼ˆå‡å°‘atomicæ“ä½œçš„æ¬¡æ•°ï¼‰ã€‚



## Lecture 6: Parallel Pattern

### All Pairs N-Body

å¤šä½“ä¹‹é—´çš„ç›¸äº’ä½œç”¨åŠ›ã€‚å¯ä»¥åœ¨GPU Gems3æ‰¾åˆ°ç›¸å…³çš„è®¨è®ºã€‚

- O(nlog(n)) : tree methods (barnes-hut)

- O(n) fast multipole method å¿«é€Ÿå¤šçº§ç®—æ³•, 20ä¸–çºª10å¤§ç®—æ³•ä¹‹ä¸€

é—®é¢˜å…ˆè½¬æ¢ä¸ºN*NçŸ©é˜µã€‚å¦‚æœæƒ³å¾—åˆ°`mat[i][j]`ï¼Œåˆ™éœ€è¦éå†å…¶ä»–n-1çš„å…ƒç´ (åˆ—æˆ–è¡Œ)ã€‚å¯ä»¥å‡è®¾åˆ—ä¸ºæ¥æºå€¼ï¼Œè¡Œä¸ºç›®æ ‡è¾“å‡ºå€¼ã€‚



å…ˆå°†å…¶åˆ†è§£æ›´å°çš„tile: P X P.æ ¹æ®ä¸Šé¢å‡è®¾ï¼Œæˆ‘ä»¬éœ€è¦Pä¸ªThreadæ¥è®¡ç®—è¿™ä¸ªçŸ©é˜µã€‚

å½“ç„¶shared memoryæœ‰é™ï¼ŒPä¸èƒ½å¤ªå¤§ã€‚è¯¾ç¨‹è®²è¿™ä¸ªcaseçœ‹é‡çš„æ˜¯åˆ†è§£é¢—ç²’åº¦çš„trade-offã€‚

emmï¼Œåˆ†è§£ä¹‹åï¼Œå¦‚ä½•mergeå‘¢ï¼Ÿ

æŒ‰ç…§ä¸‹å›¾çš„flowåº”è¯¥ä¸å­˜åœ¨mergeçš„é—®é¢˜ï¼Œå°±æ˜¯ä¸ªç®€å•çš„å¤šçº¿ç¨‹å¤„ç†æ¨¡å‹ï¼š

![31fig04.jpg](https://developer.nvidia.com/sites/all/modules/custom/gpugems/books/GPUGems3/elementLinks/31fig04.jpg)

ref: 

https://developer.nvidia.com/gpugems/gpugems3/part-v-physics-simulation/chapter-31-fast-n-body-simulation-cuda



### SpMV

ç¨€ç–çŸ©é˜µç›¸ä¹˜ã€‚

è¿™é‡Œæœ‰ä¸ªå–èˆï¼ŒçŸ©é˜µä¸å‘é‡ä¹˜æ³•ä¸ºä¾‹ï¼Œä¸€ä¸ªçº¿ç¨‹åº”è¯¥å¤„ç†å¤šå°‘çš„çŸ©é˜µæ•°æ®ï¼Œå¯ä»¥ä¸ºä¸€ä¸ªæ•°æ®ï¼Œä¹Ÿå¯ä»¥ä¸ºä¸€è¡Œæ•°æ®ã€‚ä¸¤ä¸ªéƒ½æ˜¯å¯¹çš„ï¼Œè§†æƒ…å†µè€Œå®šã€‚

å‰è€…çº¿ç¨‹æ•°é‡æ›´å¤šï¼Œçº¿ç¨‹é—´æ•°æ®äº¤æ¢æ›´å¤šï¼Œè€Œåè€…å•ä¸ªçº¿ç¨‹è®¡ç®—é‡æ›´å¤§ã€‚



çº¿ç¨‹æ›´å¤šçš„è®¡ç®—é‡å¸¦æ¥çš„é—®é¢˜æ˜¯ï¼Œè®¡ç®—é‡ä¸å‡è¡¡çš„æƒ…å†µä¸‹ï¼Œå…¶ä»–çº¿ç¨‹ä¸ºäº†ç­‰å¾…æœ€å¤§è®¡ç®—é‡çº¿ç¨‹è€Œæ¶ˆè€—å¤§é‡çš„ç­‰å¾…æ—¶é—´ã€‚

å¯¹äºçŸ©é˜µä¸å‘é‡ä¹˜æ³•ä¸ºä¾‹ï¼Œå¦‚æœæ˜¯ä¸€ä¸ªçº¿ç¨‹å¤„ç†å•ä¸ªçŸ©é˜µæ•°æ®ï¼Œéœ€è¦å¢åŠ scanæ“ä½œæ¥å¯¹ç»“æœè¿›è¡Œæ±‡æ€»ã€‚ä½†æ¯ä¸ªçº¿ç¨‹çš„è®¡ç®—é‡æ˜¯å‡è¡¡çš„ï¼Œæ€§èƒ½è¡¨ç°ä¸»è¦å–å†³äºçŸ©é˜µæ•°æ®çš„å¤§å°ã€‚



å½“ç„¶è¿˜æœ‰æ··åˆçš„æ–¹æ³•ï¼Œéœ€è¦å¯¹ç¨€ç–çŸ©é˜µçš„é›†ä¸­åº¦åˆ†å¸ƒæœ‰ç€å¾ˆå¥½çš„åˆ¤æ–­ã€‚



### BFS

bfsé‡Œé¢çš„æ·±åº¦(level)çš„æ¦‚å¿µä¸å†èµ˜è¿°ã€‚

èŠ‚ç‚¹æ•°é‡ä¸ºN, å›¾çš„æœ€å¤§æ·±åº¦ä¸ºN-1, æœ€å°æ·±åº¦ä¸º1ã€‚å…¶å®è¿™ä¸ªå€¼å’Œé—®é¢˜çš„å¹¶è¡Œåº¦ç›¸å…³ï¼Œé“¾è¡¨çš„BFS searchçš„å¹¶è¡ŒåŠ é€Ÿå¾ˆå·®ã€‚



CUDA ç‰ˆçš„BFS

- part1: åˆå§‹åŒ–visitedèŠ‚ç‚¹çŸ©é˜µï¼Œèµ·ç‚¹æ·±åº¦è®¾ä¸º0ï¼Œæœªè®¿é—®çš„èŠ‚ç‚¹æ·±åº¦ä¸º-1ï¼›
- part2: æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€æ¡è¾¹ï¼Œè¿­ä»£ä¸€æ¬¡ä»£è¡¨ä¸€æ¬¡æ·±åº¦é€’å¢ï¼Œä¾æ®source nodeçš„æ·±åº¦ï¼ŒæŠŠdestination nodeçš„èŠ‚ç‚¹æ·±åº¦å€¼éƒ½æ›´æ–°ä¸‹ï¼›
- part3: è¿­ä»£part2ï¼Œæ‰¾åˆ°ç»ˆç‚¹

ä¸€äº›é—®é¢˜ï¼š

- hostå¦‚ä½•ç»“æŸå¾ªç¯ï¼Ÿä¼ å…¥ä¸€ä¸ªæŒ‡é’ˆflag(æ¯è¿­ä»£ä¾æ¬¡ä»deviceä¼ å›host), æˆ–è®¾ç½®max iteration é™åˆ¶
- ä¸¤ä¸ªæ·±åº¦ä¸º2çš„èŠ‚ç‚¹éƒ½æŒ‡å‘åŒä¸€ä¸ªæ·±åº¦ä¸º3çš„ç‚¹ï¼Œä¼šä¸ä¼šæœ‰data race? no~



### PS6 å›¾ç‰‡æ‹¼æ¥

åˆ†ä¸¤éƒ¨åˆ†è®¨è®ºï¼šç®—æ³•å’ŒCUDAå®ç°

**ç®—æ³•ï¼š**è¯¥é—®é¢˜å°±æ˜¯å°†ä¸€å¼ è¾ƒå°çš„å›¾ç‰‡èå…¥åˆ°æºå›¾ç‰‡ä¸­ã€‚é¦–å…ˆï¼Œå¦‚ä½•æ‰¾åˆ°è¦èåˆçš„ä½ç½®ï¼Ÿemmï¼Œä¸å±äºè¿™ä¸ªä½œä¸šçš„å†…å®¹ï¼Œæä¾›çš„ä¸¤ä¸ªå›¾ç‰‡æ˜¯å¯¹é½çš„ã€‚é‚£å‰©ä¸‹å°±æ˜¯ç¡®å®šä¸‰ä¸ªåŒºåŸŸï¼Œæ ¸å¿ƒåŒºåŸŸï¼ˆè¿™ä¸ªdestImagçš„æƒé‡æ›´å¤§ï¼‰ï¼Œè¾¹ç•ŒåŒºåŸŸï¼ˆè¿™ä¸ªä¸å¥½è¯´ï¼Œå¹³å‡å§ï¼‰ï¼ŒèƒŒæ™¯å›¾åƒåŒºåŸŸï¼ˆè¿™ä¸ªå®Œå…¨æ˜¯sourceå›¾ç‰‡çš„å€¼ï¼‰ï¼Œç„¶åè¿­ä»£å¹³å‡ä¸‹æ¢¯åº¦å³å¯ã€‚

**CUDAå®ç°**ï¼šå‚è€ƒrefernce_calc.cppä¸­çš„å®ç°ï¼Œå†™ä¸ªæ²¡ç»è¿‡ä¼˜åŒ–çš„CUDAç‰ˆæœ¬ä¸éš¾ã€‚



ä¼˜åŒ–æ€è·¯ï¼š

æ³Šæ¾è¿­ä»£ç±»ä¼¼äºåœ¨maskå†…è¿›è¡Œå·ç§¯æ“ä½œï¼ŒCUDAå·ç§¯æ“ä½œçš„ä¼˜åŒ–å¯ä»¥å‚è€ƒPMPP chapter ?



### More about BFS

ç®€è¦ä»‹ç»äº†ä¸‹åŸºäºCSRå‹ç¼©æ•°æ®ç»“æ„ä¸‹çš„bfsã€‚æ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚



### List Ranking

åˆ—è¡¨æ’åºï¼š

ref: https://cs.msutexas.edu/~ranette/Dissertation/Chapter%202.pdf

æ¯”è¾ƒå¤è€è€Œä¸”ç”¨å¤„ä¸å¤ªå¤§ï¼Œå¯è§†ä¸ºlecturerçš„ç§è´§ã€‚



### Hash Table

Cuckoo Hash: è´ªå¿ƒåˆ†é…ï¼Œç„¶åæ›¿æ¢ç¢°æ’çš„é”®å€¼å¯¹ï¼Œä¸æ–­è¿­ä»£å¯»æ‰¾è§£ã€‚

ä½†ä¸ä¸€å®šæœ‰è§£ï¼Œçœ‹æ¦‚ç‡ã€‚GPUé€‚åˆè¿™ç§æ— åºçš„æ•£åˆ—è¡¨ï¼Œè®©æ‰€æœ‰çš„çº¿ç¨‹å¿™ç¢Œåœ°å»è¿­ä»£æ‰¾bucketï¼Œè€Œåƒæ ‘ã€æ’åºäºŒåˆ†è¿™ç±»æ›´é€‚åˆåœ¨CPUä¸Šå¤„ç†ã€‚



## Lecture 7: More Parallel Computing

### Parallel Optimization patterns

Paper: Stratton et al 2012, 7ä¸ªåŸºæœ¬å¹¶è¡Œä¼˜åŒ–çš„æŠ€æœ¯

**tech1: data layout transformation**

é‡æ–°ç»„ç»‡æ•°æ®ï¼Œä¾‹å¦‚ä¸‹é¢ä¸¤ä¸ªstruct:

```c
struct f{
  float a;
} A[8];

struct f{
  float a[8];
} A;
```

**tech 2 scatter to gather transformation**

åŒºåˆ†ä»£ç åœ¨å†…å­˜ä¸Šæ˜¯è¿›è¡Œå‘æ•£æ“ä½œï¼Œè¿˜æ˜¯æ”¶é›†æ“ä½œï¼Œä¸€èˆ¬åè€…çš„æ›´å¤šçš„è¯»æ“ä½œï¼Œæ˜¾ç„¶é€Ÿåº¦æ›´å¿«ã€‚



**tech3: tiling**

åˆ©ç”¨å…±äº«å†…å­˜cacheä¸€ä¸‹æ•°æ®ç„¶åè¿›è¡Œåˆ†å‘ï¼Œé¿å…å…¨å±€æ€§çš„ç›´æ¥æ‹·è´æˆ–ç§»åŠ¨æ•°æ®ã€‚



**tech4: privatization**

ä¾‹å¦‚histogramï¼Œåœ¨reduceæ“ä½œæ—¶(atomicAdd)å¯ä»¥é™åˆ¶local memoryä¸Šè¿›è¡Œè§„çº¦æ“ä½œã€‚



**tech 5 Binning/ spatial data structures**

> binning: build data structure that maps output locations to a small subset of the relevant input data

ä¾‹å¦‚äºŒç»´è½¨è¿¹å¸¦ç¢°æ’æ£€æµ‹ï¼Œå…ˆä½¿ç”¨æœ€å¤§footprintè¾¹é•¿åˆ›å»ºä¸€ä¸ªç½‘æ ¼ï¼Œå°†è½¨è¿¹ç‚¹éƒ½æ˜ å°„åˆ°è¿™ä¸ªç½‘æ ¼ä¸­ï¼Œè®¡ç®—ç‚¹ä¸ç‚¹ä¹‹é—´ç¢°æ’æ—¶ï¼Œå¯å¿½ç•¥éç›¸é‚»ç½‘æ ¼çš„ç‚¹ã€‚



**tech6 compaction**

ä¾‹å¦‚CSRè¡¨ç¤ºç¨€ç–çŸ©é˜µï¼Œæˆ–è¿›è¡Œmaskè¿‡æ»¤ä¸€ä¸ªæ•°ç»„ã€‚



**tech7 regularization**

æ­£åˆ™ä¸»è¦æ˜¯ä¸ºäº†load balance,å¯¹äºæ•°æ®åˆ†å¸ƒæ¯”è¾ƒæ¸…æ™°çš„inputè¾ƒä¸ºæœ‰æ•ˆã€‚

> reorganizing input data to reduce load imbalance



### Libraries

**cuBLAS: ** çº¿æ€§ä»£æ•°æ±‚è§£åº“

**cuFFT:** FFT

**cuSPARSE:** BLAS-like routines for sparse matrix formats

**cuRAND / NPP / Magma / CULA / ArrayFire**



**CUB**

é’ˆå¯¹ç¡¬ä»¶ä¸åŒï¼Œç§»æ¤CUDAä»£ç æ—¶ï¼ŒCUBå¯ä»¥è‡ªåŠ¨åœ°è§£å†³çº¿ç¨‹å¯åŠ¨æ•°é‡ã€shared memoryå¤§å°è¿™ç±»ç¡¬ä»¶ç›¸å…³åœ°å‚æ•°ã€‚

**CudaDMA**

> template library designed to :
>
> - make it easier to use shared memory
> - at high performance





### Programming power tools

PyCUDA, Copperhead(generates thrust code), CUDA Fortran

è·¨å¹³å°ï¼š

OpenCL, OpenGL Compute, OpenACCï¼ˆåŸºäºæŒ‡ä»¤é›†ï¼Œç›¸å½“äºåŠ å¼ºç‰ˆæœ¬åœ°OpenMPï¼‰ã€‚



### Dynamic Parallelism

bulk parallelism: ä¸€å¯¹ä¸€,æ¯ä¸ªçº¿ç¨‹åšåŒæ ·åœ°äº‹æƒ…ã€‚

Nested parallelism: åµŒå¥—åœ°å¹¶è¡Œï¼Œ

task / recursive parallelism 



dynamic parallelism 

æ¯ä¸ªçº¿ç¨‹éƒ½å¯å¯åŠ¨kernelã€‚å¯¹äºåƒé€’å½’è¿™ç§å°±å¾ˆæœ‰ç”¨ï¼Œå¯ä»¥åœ¨kernelä¸­å¯åŠ¨æ›´å°çš„kernel.



## Review

é‡è¦æ€§ï¼šâ­â­

éš¾åº¦ï¼šâ­â­

è¯¾ç¨‹è´¨é‡ï¼šâ­â­â­â­

å…ˆè°ˆç¼ºç‚¹ï¼š

- è¯¥è¯¾ç¨‹æ¶µç›–çš„åªæ˜¯Parallel Computingä¸­ä¸€å°éƒ¨åˆ†å†…å®¹ï¼Œå¯¹äºå­¦ä¹ CUDAæœ‰ç€å¾ˆå¥½çš„å¯å‘ï¼Œä½†è¿˜æœ‰å¾ˆå¤šå¹¶è¡Œè®¡ç®—ä»¥åŠGPUç¼–ç¨‹å†…å®¹éœ€è¦è¿›ä¸€æ­¥æ·±å…¥ã€‚åŒæ—¶ï¼Œè¯¥è¯¾ç¨‹å¯¹æœºå™¨å­¦ä¹ CUDAåŠ é€Ÿç›¸å…³çš„å†…å®¹ä¹Ÿæ˜¯ç¼ºå¤±çš„ï¼Œéœ€è¦è‡ªå·±é¢å¤–è¡¥å……ï¼›
- ä½œä¸šéš¾åº¦ä¸€èˆ¬ï¼Œæ²¡æœ‰autograder;
- æ‰‹å†™PPTï¼Œå†…å®¹ä¸æ˜¯å¾ˆä¸°å¯Œã€‚è¯¾ç¨‹è¦è®²çš„æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„é¢˜ç›®ï¼šparallel computingï¼Œä½†è®²çš„å†…å®¹æ˜¯è¿™ä¸€é¢†åŸŸçš„ä¸€å°éƒ¨åˆ†ã€‚

ä¼˜ç‚¹æ˜¯ç®€å•ï¼Œå¯¹CUDAçš„ä¸€äº›èŒƒå¼åŸºæœ¬éƒ½æ¶µç›–åˆ°äº†ï¼Œä½œä¸ºCUDAçš„å…¥é—¨è¯¾éå¸¸åˆé€‚ã€‚







